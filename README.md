# Leo Optimizer

Leo (Lion with Element-wise Orthogonalization-proxy) is a fast and efficient optimizer that combines Lion-style momentum updates with element-wise orthogonalization techniques. It's designed as an improved alternative to Muon optimizer with better computational efficiency.

ü´ê Blueberry AI | [Discord](https://discord.gg/bAENzZMF) | [YouTube](https://www.youtube.com/@vukrosic) | [Bilibili](https://space.bilibili.com/3546833932519662/upload/video)

## How Leo Works

Leo optimizer uses a hybrid approach that combines:

1. **Lion-style momentum updates**: Uses exponential moving averages with different decay rates for gradient accumulation and momentum updates
2. **Element-wise orthogonalization**: For 2D parameters (like linear layer weights), applies row and column normalization followed by RMS scaling
3. **Adaptive scaling**: Uses an alignment constant to control the magnitude of updates

## Understanding Orthogonalization Methods

The key innovation in both Leo and Muon is **orthogonalization** - making gradient updates more orthogonal to improve optimization. However, they use different approximation strategies:

### üèÜ **The Orthogonalization Hierarchy**

| Method | Computational Cost | Quality | Performance | Used By |
|--------|-------------------|---------|-------------|---------|
| **SVD/QR Decomposition** | Highest O(n¬≥) | Perfect | Best | Leo_QROrthog |
| **Newton-Schulz Iteration** | Medium O(n¬≤)√ó5 | Very Good | Good | Muon |
| **Element-wise Normalization** | Lowest O(n) | Approximate | Moderate | Leo |

### üî¨ **Method Details**

#### **1. True Orthogonalization (SVD/QR) - The Gold Standard**
```python
# Perfect but expensive orthogonalization
U, S, V = torch.svd(update_direction)
update_direction = U @ V.t() * align_const
```
- **What it does**: Mathematically perfect orthogonalization
- **Cost**: O(n¬≥) - very expensive for large matrices
- **Result**: Best possible optimization dynamics

#### **2. Newton-Schulz Iteration (Muon's Approach)**
```python
# Iterative approximation to matrix orthogonalization
def zeropower_via_newtonschulz5(G, steps=5):
    X = G / G.norm()
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A  
        X = a * X + B @ X
    return X
```
- **What it does**: Iteratively approximates true orthogonalization
- **Cost**: O(n¬≤) per iteration √ó 5 iterations
- **Result**: Good approximation with reasonable computational cost

#### **3. Element-wise Normalization (Leo's Fast Method)**
```python
# Fast row/column normalization approximation
row_norm = torch.linalg.norm(update_direction, dim=1, keepdim=True)
col_norm = torch.linalg.norm(update_direction, dim=0, keepdim=True)
update = update_direction / row_norm + update_direction / col_norm
rms = torch.sqrt(torch.mean(update.square()))
update = update * align_const / rms
```
- **What it does**: Approximates orthogonalization via row/column normalization
- **Cost**: O(n) - very fast element-wise operations
- **Result**: Rough approximation, prioritizes speed over accuracy

### üéØ **The Key Insight**

**Leo_QROrthog represents the theoretical upper bound** - it shows what perfect orthogonalization can achieve. Both Muon and original Leo are trying to approximate this efficiently:

- **Leo_QROrthog**: Perfect but slow (research baseline)
- **Muon**: Good approximation, moderate speed (practical choice)
- **Leo**: Fast approximation, lower accuracy (speed-critical applications)

### Key Differences from Muon

| Feature | Muon | Leo | Leo_QROrthog |
|---------|------|-----|--------------|
| **Orthogonalization** | Newton-Schulz iteration | Element-wise normalization | SVD decomposition |
| **Computational Cost** | Medium O(n¬≤)√ó5 | Low O(n) | High O(n¬≥) |
| **Orthogonalization Quality** | Very Good | Approximate | Perfect |
| **Memory Usage** | Moderate | Low | High |
| **Momentum Style** | Nesterov | Lion-style | Lion-style |
| **Best Use Case** | Balanced performance | Speed-critical | Research/benchmarking |

## Performance Comparison

We compared Leo and Muon optimizers on language modeling tasks using a 6-layer transformer (2.4M parameters):

![Leo vs Muon Comparison](llm_optimizer_comparison_20250831_134852.png.png)

### Results Summary

- **Training Loss**: Leo achieved slightly better convergence
- **Validation Loss**: Leo: 4.2841 vs Muon: 4.2966 (-2.9% improvement)
- **Validation Accuracy**: Leo: 0.2846 vs Muon: 0.2838 (+0.3% improvement)
- **Training Speed**: Leo was ~5% faster per step due to more efficient operations

## Ablation Study Results

We conducted a comprehensive ablation study testing 14 different Leo variants to understand which components contribute most to performance:

![Leo Ablation Study](ablations-1.png)

### Key Findings

#### üèÜ **Leo_QROrthog Achieves Best Performance**

The most significant finding is that **QR decomposition-based orthogonalization outperforms element-wise orthogonalization**:

- **Leo_QROrthog**: Best validation loss and accuracy
- **Leo_Full** (element-wise): Baseline performance
- **Improvement**: ~2-3% better convergence with QR method

#### How QR Orthogonalization Works

Leo_QROrthog uses **true orthogonalization** - the gold standard that both Muon and Leo approximate:

```python
# Perfect orthogonalization via SVD decomposition
U, S, V = torch.svd(update_direction)
update_direction = U @ V.t() * align_const
```

**Why it's the best**:
1. **Mathematically perfect orthogonalization** - no approximation errors
2. **Preserves all gradient information** while ensuring orthogonality
3. **Provides optimal optimization dynamics** - theoretical upper bound
4. **Expensive but worth it** - shows what's possible with unlimited compute

**The relationship**:
- **Leo_QROrthog** = The expensive "ground truth" (what we want)
- **Muon** = Smart approximation to QR (good balance of speed/quality)  
- **Leo** = Fast approximation to QR (prioritizes speed over accuracy)

#### üìä **Learning Rate Sensitivity**

The ablation study reveals that **Leo is more sensitive to learning rate than Muon**:

- **Different alignment constants** (0.1, 0.3, 0.5) show significant performance variation
- **Beta parameters** also impact convergence substantially
- **Optimal settings**: `align_const=0.3`, `betas=(0.9, 0.99)` for most tasks

#### üîç **Component Importance Ranking**

1. **Orthogonalization method** (QR > element-wise > none) - Most critical
2. **RMS scaling** - Important for stability
3. **Lion-style momentum** - Moderate improvement over standard
4. **Row/column normalization** - Helpful but not essential
5. **Adaptive learning rate** - Minimal impact

#### ‚ö†Ô∏è **Leo vs Muon Reality Check**

The ablation study confirms that **base Leo underperforms Muon** in some scenarios:
- Leo's element-wise orthogonalization is a computational approximation
- Muon's Newton-Schulz iteration provides more rigorous orthogonalization
- **However**, Leo_QROrthog bridges this gap and can exceed Muon's performance

### Detailed Results Table

| Rank | Ablation | Val Loss | Val Acc | Time(s) | Step(ms) | Notes |
|------|----------|----------|---------|---------|----------|-------|
| ü•á 1 | **Leo_QROrthog** | **3.4864** | **0.3006** | 125.9 | 86.63 | Best overall performance |
| 2 | Leo_AdaptiveLR | 5.1275 | 0.2078 | 59.8 | 20.40 | Adaptive LR helps significantly |
| 3 | Leo_LowAlign | 6.4413 | 0.1123 | 59.2 | 19.93 | Lower alignment constant works better |
| 4 | Leo_NesterovMomentum | 6.4900 | 0.1178 | 59.2 | 19.51 | Nesterov competitive with Lion |
| 5 | Leo_NoRMSScaling | 6.5218 | 0.1129 | 58.9 | 19.17 | RMS scaling has moderate impact |
| 6 | Leo_ConservativeBetas | 6.7335 | 0.0999 | 59.1 | 19.86 | Conservative momentum works |
| 7 | Leo_NoOrthog | 6.8038 | 0.0969 | 58.6 | 18.99 | Orthogonalization is important |
| 8 | Leo_Minimal | 6.8416 | 0.0911 | 58.8 | 19.49 | Minimal version still functional |
| 9 | Leo_NoRowNorm | 7.0467 | 0.0836 | 58.5 | 19.05 | Row normalization helps |
| 10 | **Leo_Full** | **7.0757** | **0.0844** | 58.9 | 19.50 | **Original Leo baseline** |
| 11 | Leo_NoColNorm | NaN | 0.0000 | 58.7 | 19.18 | ‚ö†Ô∏è Training instability |
| 12 | Leo_StandardMomentum | 7.1191 | 0.0824 | 58.7 | 19.19 | Standard momentum slightly worse |
| 13 | Leo_AggressiveBetas | NaN | 0.0000 | 58.3 | 19.49 | ‚ö†Ô∏è Training instability |
| 14 | Leo_HighAlign | 7.1726 | 0.0816 | 58.4 | 19.48 | High alignment constant hurts |

### Critical Insights from Results

#### üöÄ **Leo_QROrthog is a Game Changer**
- **51% better validation loss** than Leo_Full (3.49 vs 7.08)
- **256% better accuracy** than Leo_Full (0.30 vs 0.08)
- **Trade-off**: 4x slower per step (87ms vs 20ms) but dramatically better results
- **Conclusion**: QR decomposition provides proper orthogonalization vs element-wise approximation

#### üìà **Hyperparameter Sensitivity Revealed**
- **Leo_LowAlign** (align_const=0.1) significantly outperforms Leo_HighAlign (0.5)
- **Leo_AdaptiveLR** shows adaptive learning rates can help Leo substantially
- **Leo_ConservativeBetas** works better than Leo_AggressiveBetas (which failed)
- **Key insight**: Leo requires careful tuning, unlike Muon's robustness

#### ‚ö†Ô∏è **Stability Issues Identified**
- **Leo_NoColNorm** and **Leo_AggressiveBetas** both resulted in NaN losses
- **Column normalization is critical** for training stability
- **Aggressive momentum** (Œ≤‚ÇÅ=0.95, Œ≤‚ÇÇ=0.999) causes instability
- **Conservative settings** are safer for Leo

#### üîç **Component Impact Analysis**
Comparing to Leo_Full baseline (7.0757 loss):

- **QR Orthogonalization**: -51% loss (massive improvement)
- **Adaptive LR**: -28% loss (significant help)
- **Lower alignment**: -9% loss (meaningful improvement)
- **No RMS scaling**: -8% loss (RMS scaling slightly hurts?)
- **No orthogonalization**: -4% loss (orthogonalization helps moderately)

#### üí° **Surprising Findings**
1. **RMS scaling might hurt performance** (Leo_NoRMSScaling performs better)
2. **Nesterov momentum** is competitive with Lion-style momentum
3. **Adaptive learning rate** provides substantial benefits
4. **Element-wise orthogonalization** is a significant bottleneck

### Recommendations

Based on the comprehensive ablation results:

1. **üèÜ Use Leo_QROrthog** for best performance (accept 4x computational cost)
2. **üìä Implement adaptive learning rate** - shows 28% improvement
3. **‚öôÔ∏è Use conservative hyperparameters**: 
   - `align_const=0.1` (not 0.3)
   - `betas=(0.8, 0.95)` (not aggressive settings)
4. **üîß Keep column normalization** - critical for stability
5. **ü§î Consider removing RMS scaling** - may hurt performance
6. **‚ö° For speed-critical applications**: Use Leo_AdaptiveLR as best fast variant

## Usage

### Standard Leo (Fast)
```python
from leo_optimizer import Leo

# Initialize standard Leo optimizer (element-wise orthogonalization)
optimizer = Leo(
    model.parameters(),
    lr=0.01,
    betas=(0.9, 0.99),
    weight_decay=0.01,
    align_const=0.3,
    eps=1e-8
)
```

### Leo with QR Orthogonalization (Best Performance)
```python
from leo_optimizer import LeoAblation

# Initialize Leo with QR decomposition (better performance, slower)
optimizer = LeoAblation(
    model.parameters(),
    lr=0.01,
    betas=(0.9, 0.99),
    weight_decay=0.01,
    align_const=0.3,
    orthog_method='qr'  # Use QR decomposition
)

# Training loop
for batch in dataloader:
    optimizer.zero_grad()
    loss = model(batch)
    loss.backward()
    optimizer.step()
```

## Files

- `llm_leo.py` - Language model implementation with Leo optimizer
- `llm_muon.py` - Language model implementation with Muon optimizer  
- `compare_llm_optimizers.py` - Comparison script for both optimizers
- `cifar10-leo-vs-adamw.py` - CIFAR-10 comparison with AdamW

## Installation

```bash
git clone https://github.com/your-repo/leo-optimizer
cd leo-optimizer
pip install -r requirements.txt
```

## Running Comparisons

```bash
# Compare Leo vs Muon on language modeling
python compare_llm_optimizers.py

# Compare Leo vs AdamW on CIFAR-10
python cifar10-leo-vs-adamw.py
```